{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanyakiran/Future-of-DRUG-Design-with-ReinforcementLearning/blob/main/Python_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVT11Y8CbAu"
      },
      "source": [
        "# Welcome"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Molecule_Protein.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1xV32BbH8poCWTn2N_Dbc248xP73odAP7\n",
        "\"\"\"\n",
        "\n",
        "!pip install rdkit Chem\n",
        "\n",
        "!pip install torch torch_geometric\n",
        "\n",
        "!pip install DataLoader\n",
        "\n",
        "!pip install Data\n",
        "\n",
        "# import packages\n",
        "\n",
        "# general tools\n",
        "import numpy as np\n",
        "\n",
        "# RDkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
        "\n",
        "# Pytorch and Pytorch Geometric\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels=16, out_channels=32)\n",
        "        self.conv2 = GCNConv(in_channels=32, out_channels=64)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "gnn_model = GNNModel()\n",
        "\n",
        "# Example using PyTorch\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Assume 'data' is a graph data object containing features and edge_index\n",
        "gnn_model.train()\n",
        "optimizer.zero_grad()\n",
        "output = gnn_model(data)  # Forward pass\n",
        "loss = criterion(output, data.y)  # Compute loss\n",
        "loss.backward()  # Backpropagate\n",
        "optimizer.step()  # Update weights\n",
        "\n",
        "\"\"\"**Atom Featurization**\"\"\"\n",
        "\n",
        "def one_hot_encoding(x, permitted_list):\n",
        "    \"\"\"\n",
        "    Maps input elements x which are not in the permitted list to the last element\n",
        "    of the permitted list.\n",
        "    \"\"\"\n",
        "\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "\n",
        "    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n",
        "\n",
        "    return binary_encoding\n",
        "\n",
        "def get_atom_features(atom,\n",
        "                      use_chirality = True,\n",
        "                      hydrogens_implicit = True):\n",
        "    \"\"\"\n",
        "    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n",
        "    \"\"\"\n",
        "\n",
        "    # define list of permitted atoms - 43\n",
        "\n",
        "    permitted_list_of_atoms =  ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca','Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n",
        "\n",
        "    if hydrogens_implicit == False:\n",
        "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "    # compute atom features\n",
        "\n",
        "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "\n",
        "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])  #5\n",
        "\n",
        "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"]) #7\n",
        "\n",
        "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "\n",
        "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "\n",
        "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "\n",
        "    atomic_mass_scaled = [float((atom.GetMass() - 10.812)/116.092)]\n",
        "\n",
        "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)]\n",
        "\n",
        "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)]\n",
        "\n",
        "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
        "\n",
        "    if use_chirality == True:\n",
        "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "        atom_feature_vector += chirality_type_enc\n",
        "\n",
        "    if hydrogens_implicit == True:\n",
        "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "    return np.array(atom_feature_vector)\n",
        "\n",
        "\"\"\"**Bond Featurization**\"\"\"\n",
        "\n",
        "def get_bond_features(bond,\n",
        "                      use_stereochemistry = True):\n",
        "    \"\"\"\n",
        "    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n",
        "    \"\"\"\n",
        "\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry == True:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)\n",
        "\n",
        "\"\"\"**Generating labeled Pytorch Geometric Graph Objects**\"\"\"\n",
        "\n",
        "def create_pytorch_geometric_graph_data_list_from_smiles_and_labels(x_smiles, y):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "\n",
        "    x_smiles = [smiles_1, smiles_2, ....] ... a list of SMILES strings\n",
        "    y = [y_1, y_2, ...] ... a list of numerial labels for the SMILES strings (such as associated pKi values)\n",
        "\n",
        "    Outputs:\n",
        "\n",
        "    data_list = [G_1, G_2, ...] ... a list of torch_geometric.data.Data objects which represent labeled molecular graphs that can readily be used for machine learning\n",
        "\n",
        "    \"\"\"\n",
        "    x_smiles=['CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(C)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)\\C=C\\CF)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(c12)C(F)(F)F',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C(F)=C)c1cccc2cccc(Cl)c12',\n",
        "'C[C@@H]1CN2[C@H](CN1C(=O)C=C)C(=O)N(C)c1cnc3c(F)c(c(Cl)cc3c21)-c1c(O)cccc1F',\n",
        "'CCc1cccc2cccc(N3CCc4c(C3)nc(OC[C@@H]3CCCN3C)nc4N3CCN([C@@H](CC#N)C3)C(=O)C=C)c12',\n",
        "'CC(C)c1ccccc1-n1c2cc(c(Cl)cc2c(nc1=O)N1CCN(CC1)C(=O)C=C)-c1c(C)ccc2[nH]ncc12',\n",
        "'COC\\C=C\\C(=O)N1CCN(C[C@@H]1CC#N)c1nc(OC[C@@H]2CCCN2C)nc2CN(CCc12)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2ccccc12']\n",
        "    y=[1,1,1,1,1,1,1,1,1,1]\n",
        "    data_list = []\n",
        "\n",
        "    for (smiles, y_val) in zip(x_smiles, y):\n",
        "\n",
        "        # convert SMILES to RDKit mol object\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        # get feature dimensions\n",
        "        n_nodes = mol.GetNumAtoms()\n",
        "        n_edges = 2*mol.GetNumBonds()\n",
        "        unrelated_smiles = \"O=O\"\n",
        "        unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n",
        "        n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n",
        "        n_edge_features = len(get_bond_features(unrelated_mol.GetBondBetweenAtoms(0,1)))\n",
        "\n",
        "        # construct node feature matrix X of shape (n_nodes, n_node_features)\n",
        "        X = np.zeros((n_nodes, n_node_features))\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
        "\n",
        "        X = torch.tensor(X, dtype = torch.float)\n",
        "\n",
        "        # construct edge index array E of shape (2, n_edges)\n",
        "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
        "        torch_rows = torch.from_numpy(rows.astype(np.int64)).to(torch.long)\n",
        "        torch_cols = torch.from_numpy(cols.astype(np.int64)).to(torch.long)\n",
        "        E = torch.stack([torch_rows, torch_cols], dim = 0)\n",
        "\n",
        "        # construct edge feature array EF of shape (n_edges, n_edge_features)\n",
        "        EF = np.zeros((n_edges, n_edge_features))\n",
        "\n",
        "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
        "\n",
        "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
        "\n",
        "        EF = torch.tensor(EF, dtype = torch.float)\n",
        "\n",
        "        # construct label tensor\n",
        "        y_tensor = torch.tensor(np.array([y_val]), dtype = torch.float)\n",
        "\n",
        "        # construct Pytorch Geometric data object and append to data list\n",
        "        data_list.append(Data(x = X, edge_index = E, edge_attr = EF, y = y_tensor))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\"\"\"**Training Loop and Summary(GNNs)**\"\"\"\n",
        "\n",
        "# canonical training loop for a Pytorch Geometric GNN model gnn_model\n",
        "x_smiles=['CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(C)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)\\C=C\\CF)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(c12)C(F)(F)F',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C(F)=C)c1cccc2cccc(Cl)c12',\n",
        "'C[C@@H]1CN2[C@H](CN1C(=O)C=C)C(=O)N(C)c1cnc3c(F)c(c(Cl)cc3c21)-c1c(O)cccc1F',\n",
        "'CCc1cccc2cccc(N3CCc4c(C3)nc(OC[C@@H]3CCCN3C)nc4N3CCN([C@@H](CC#N)C3)C(=O)C=C)c12',\n",
        "'CC(C)c1ccccc1-n1c2cc(c(Cl)cc2c(nc1=O)N1CCN(CC1)C(=O)C=C)-c1c(C)ccc2[nH]ncc12',\n",
        "'COC\\C=C\\C(=O)N1CCN(C[C@@H]1CC#N)c1nc(OC[C@@H]2CCCN2C)nc2CN(CCc12)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2ccccc12']\n",
        "y=[1,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "# create list of molecular graph objects from list of SMILES x_smiles and list of labels y\n",
        "data_list = create_pytorch_geometric_graph_data_list_from_smiles_and_labels(x_smiles, y)\n",
        "\n",
        "# create dataloader for training\n",
        "dataloader = DataLoader(dataset = data_list, batch_size = 2**7)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# define optimiser\n",
        "optimiser = torch.optim.Adam(gnn_model.parameters(), lr = 1e-3)\n",
        "\n",
        "# loop over 10 training epochs\n",
        "for epoch in range(10):\n",
        "\n",
        "    # set model to training mode\n",
        "    gnn_model.train()\n",
        "\n",
        "    # loop over minibatches for training\n",
        "    for (k, batch) in enumerate(dataloader):\n",
        "\n",
        "        # compute current value of loss function via forward pass\n",
        "        output = gnn_model(batch)\n",
        "        loss_function_value = loss_function(output[:,0], torch.tensor(batch.y, dtype = torch.float32))\n",
        "\n",
        "        # set past gradient to zero\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # compute current gradient via backward pass\n",
        "        loss_function_value.backward()\n",
        "\n",
        "        # update model weights using gradient and optimisation method\n",
        "        optimiser.step()\n",
        "\n",
        "\n",
        "\n",
        "!pip install rdkit\n",
        "\n",
        "!pyenv(\"Version\",\"C:\\Users...\\python.exe\");\n",
        "\n",
        "\"\"\"How to neglect/eliminate the hydrogen bonds from this code?\"\"\"\n",
        "\n",
        "from rdkit.Chem import AllChem as Chem\n",
        "from rdkit.Chem import rdMolTransforms\n",
        "from rdkit.Chem.Draw import rdMolDraw2D, rdDepictor, IPythonConsole\n",
        "rdDepictor.SetPreferCoordGen(True)\n",
        "from IPython.display import Image\n",
        "\n",
        "# smiles = data[\"smiles\"][5:6].values\n",
        "smiles = ['CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(C)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)\\C=C\\CF)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(c12)C(F)(F)F',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C(F)=C)c1cccc2cccc(Cl)c12',\n",
        "'C[C@@H]1CN2[C@H](CN1C(=O)C=C)C(=O)N(C)c1cnc3c(F)c(c(Cl)cc3c21)-c1c(O)cccc1F',\n",
        "'CCc1cccc2cccc(N3CCc4c(C3)nc(OC[C@@H]3CCCN3C)nc4N3CCN([C@@H](CC#N)C3)C(=O)C=C)c12',\n",
        "'CC(C)c1ccccc1-n1c2cc(c(Cl)cc2c(nc1=O)N1CCN(CC1)C(=O)C=C)-c1c(C)ccc2[nH]ncc12',\n",
        "'COC\\C=C\\C(=O)N1CCN(C[C@@H]1CC#N)c1nc(OC[C@@H]2CCCN2C)nc2CN(CCc12)c1cccc2cccc(Cl)c12',\n",
        "'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2ccccc12'] #['CN1C=NC2=C1C(=O)N(C(=O)N2C)C']\n",
        "\n",
        "# 3D for lengths\n",
        "mol3d = Chem.AddHs(Chem.MolFromSmiles(smiles[0]))\n",
        "Chem.EmbedMolecule(mol3d, randomSeed=42)\n",
        "Chem.MMFFOptimizeMolecule(mol3d)\n",
        "\n",
        "bonds = [(x.GetBeginAtomIdx(), x.GetEndAtomIdx()) for x in mol3d.GetBonds()]\n",
        "\n",
        "conf = mol3d.GetConformer()\n",
        "\n",
        "b_lengths = []\n",
        "b_angles = []\n",
        "\n",
        "for b in bonds:\n",
        "    bl = rdMolTransforms.GetBondLength(conf,b[0],b[1]) #GetAngleDeg\n",
        "    b_lengths.append(str(round(bl,2)))\n",
        "\n",
        "\n",
        "print(len(b_lengths))\n",
        "print(b_lengths)\n",
        "print(len(bonds))\n",
        "print(bonds)\n",
        "\n",
        "mol3d_without_Hs = Chem.RemoveHs(mol3d)\n",
        "\n",
        "bonds = [(x.GetBeginAtomIdx(), x.GetEndAtomIdx()) for x in mol3d_without_Hs.GetBonds()]\n",
        "\n",
        "conf = mol3d_without_Hs.GetConformer()\n",
        "\n",
        "b_lengths = []\n",
        "b_angles = []\n",
        "\n",
        "for b in bonds:\n",
        "    bl = rdMolTransforms.GetBondLength(conf,b[0],b[1]) #GetAngleDeg\n",
        "    b_lengths.append(str(round(bl,2)))\n",
        "\n",
        "for bnd in mol3d_without_Hs.GetBonds():\n",
        "    bl = rdMolTransforms.GetBondLength(conf,bnd.GetBeginAtomIdx(),bnd.GetEndAtomIdx())\n",
        "    # set the bond note on the molecule, which we'll draw:\n",
        "    mol3d_without_Hs.GetBondWithIdx(bnd.GetIdx()).SetProp('bondNote',f'{bl:.2f}')\n",
        "\n",
        "\"\"\"Chemistry-Convert-SMILES_to_Molecular_Graph\"\"\"\n",
        "\n",
        "!git clone https://github.com/mathworks/Chemistry-Convert-SMILES_to_Molecular_Graph.git\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %cd Chemistry-Convert-SMILES_to_Molecular_Graph/\n",
        "# %ls\n",
        "\n",
        "!python ChemMolFromSmiles.py\n",
        "\n",
        "!python echo_env.py\n",
        "\n",
        "!python GetNodeFeaturesMatrix.py\n",
        "\n",
        "!python GetNumAtoms.py\n",
        "\n",
        "!python GetSubstructMatch.py\n",
        "\n",
        "!python MolToPDBBlock.py\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Reading, Drawing, and Analyzing a Molecule\n",
        "\n",
        "Reading and drawing the molecule of caffeine\n",
        "\"\"\"\n",
        "\n",
        "import rdkit #RDKit ----------------------------------------------------------------\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# define the smiles string and covert it into a molecule sturcture ------------\n",
        "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "\n",
        "mol = Chem.MolFromSmiles(caffeine_smiles)\n",
        "\n",
        "# draw the modecule -----------------------------------------------------------\n",
        "Draw.MolToFile(mol, 'caffeine.png')\n",
        "\n",
        "# draw the molecule with property ---------------------------------------------\n",
        "for i, atom in enumerate(mol.GetAtoms()):\n",
        "    atom.SetProp(\"molAtomMapNumber\", str(atom.GetIdx()))\n",
        "\n",
        "Draw.MolToFile(mol, 'caffeine_with_prop.png')\n",
        "\n",
        "import rdkit #RDKit ----------------------------------------------------------------\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# define the smiles string and covert it into a molecule sturcture ------------\n",
        "caffeine_smiles = 'CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2ccccc12'\n",
        "\n",
        "mol = Chem.MolFromSmiles(caffeine_smiles)\n",
        "\n",
        "# draw the modecule -----------------------------------------------------------\n",
        "Draw.MolToFile(mol, 'caffeine1.png')\n",
        "\n",
        "# draw the molecule with property ---------------------------------------------\n",
        "for i, atom in enumerate(mol.GetAtoms()):\n",
        "    atom.SetProp(\"molAtomMapNumber\", str(atom.GetIdx()))\n",
        "\n",
        "Draw.MolToFile(mol, 'caffeine_with_prop1.png')\n",
        "\n",
        "\"\"\"Displaying the atoms and bonds in the molecule of caffeine\"\"\"\n",
        "\n",
        "import rdkit #RDKit ----------------------------------------------------------------\n",
        "from rdkit import Chem\n",
        "\n",
        "# define the smiles string and covert it into a molecule sturcture ------------\n",
        "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "\n",
        "mol = Chem.MolFromSmiles(caffeine_smiles)\n",
        "\n",
        "# print the atoms of the molecule ---------------------------------------------\n",
        "for atom in mol.GetAtoms():\n",
        "    print(atom.GetIdx(),',',\n",
        "          atom.GetAtomicNum(),',',\n",
        "          atom.GetIsAromatic(),',',\n",
        "          atom.GetSymbol())\n",
        "\n",
        "# print the bonds of the molecule ---------------------------------------------\n",
        "for bond in mol.GetBonds():\n",
        "    print(bond.GetBeginAtomIdx(),',',\n",
        "          bond.GetEndAtomIdx(),',',\n",
        "          bond.GetBondType())\n",
        "\n",
        "\"\"\"Generating Molecular Fingerprint from a SMILES String\"\"\"\n",
        "\n",
        "# import RDKit ----------------------------------------------------------------\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import RDKFingerprint\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "\n",
        "# import numpy for data type conversion ---------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "# define the smiles string and covert it into a molecule sturcture ------------\n",
        "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "\n",
        "mol = Chem.MolFromSmiles(caffeine_smiles)\n",
        "\n",
        "# retrieving RDK Fingerprint --------------------------------------------------\n",
        "fingerprint_rdk = RDKFingerprint(mol)\n",
        "print(\">>> RDK Fingerprint = \", fingerprint_rdk)\n",
        "\n",
        "fingerprint_rdk_np = np.array(fingerprint_rdk)\n",
        "print(\">>> RDK Fingerprint in numpy = \", fingerprint_rdk_np)\n",
        "print(\">>> RDK Fingerprint in numpy shape = \", fingerprint_rdk_np.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "# retrieving Morgan Fingerprint -----------------------------------------------\n",
        "fingerprint_morgan = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2)\n",
        "print(\">>> Morgan Fingerprint = \", fingerprint_morgan)\n",
        "\n",
        "fingerprint_morgan_np = np.array(fingerprint_morgan)\n",
        "print(\">>> Morgan Fingerprint in numpy : \", fingerprint_morgan_np)\n",
        "print(\">>> Morgan Fingerprint in numpy shape = \", fingerprint_morgan_np.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "\"\"\"Generating One-Hot Encoding from a SMILES string\"\"\"\n",
        "\n",
        "# import library --------------------------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "# define SMILES characters ----------------------------------------------------\n",
        "SMILES_CHARS = [' ',\n",
        "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
        "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                '=', '@',\n",
        "                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
        "                'R', 'S', 'T', 'V', 'X', 'Z',\n",
        "                '[', '\\\\', ']',\n",
        "                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
        "                't', 'u']\n",
        "\n",
        "# define encoder and decoder --------------------------------------------------\n",
        "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
        "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
        "\n",
        "def smiles_encoder( smiles, maxlen=120 ):\n",
        "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
        "    for i, c in enumerate( smiles ):\n",
        "        X[i, smi2index[c] ] = 1\n",
        "    return X\n",
        "\n",
        "def smiles_decoder( X ):\n",
        "    smi = ''\n",
        "    X = X.argmax( axis=-1 )\n",
        "    for i in X:\n",
        "        smi += index2smi[ i ]\n",
        "    return smi\n",
        "\n",
        "# get a taste of caffeine -----------------------------------------------------\n",
        "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "\n",
        "caffeine_encoding = smiles_encoder(caffeine_smiles)\n",
        "\n",
        "print(caffeine_encoding.shape) # (120, 56)\n",
        "\n",
        "!pip install mol2vec\n",
        "\n",
        "\"\"\"Generating Word Embedding from a SMILES String\"\"\"\n",
        "\n",
        "# import rdkit/mol2vec/word2vec -----------------------------------------------\n",
        "from rdkit import Chem\n",
        "from mol2vec.features import mol2alt_sentence, MolSentence, DfVec, sentences2vec\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# import numpy/pandas ---------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\">>> read the data file ... \")\n",
        "hiv = pd.read_csv('/content/bindingDB_ic50.csv')   #pd.read_csv('HIV.csv')\n",
        "print(\">>> data shape = \", hiv.shape)\n",
        "print(\">>> data columns = \", hiv.columns, \"\\n\")\n",
        "print(hiv)\n",
        "print()\n",
        "\n",
        "print(\">>> create mol from smiles ... \")\n",
        "hiv['mol'] = hiv['Ligand SMILES'].apply(lambda x: Chem.MolFromSmiles(x))\n",
        "\n",
        "print(\">>> create sentence from mol ... \")\n",
        "hiv['sentence'] = hiv.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
        "\n",
        "print(\">>> load the word2vec model ... \")\n",
        "w2v_model = word2vec.Word2Vec.load('model_300dim.pkl')\n",
        "\n",
        "print(\">>> create embedding from sentence ... \")\n",
        "hiv['embedding'] = [DfVec(x) for x in sentences2vec(hiv['sentence'], w2v_model)]\n",
        "\n",
        "print(\">>> data columns = \", hiv.columns, \"\\n\")\n",
        "\n",
        "hiv_mol2vec = np.array([x.vec for x in hiv['embedding']])\n",
        "hiv_mol2vec = pd.DataFrame(hiv_mol2vec)\n",
        "print(\">>> hiv_mol2vec shape = \", hiv_mol2vec.shape)\n",
        "print(hiv_mol2vec)\n",
        "print()\n",
        "\n",
        "from rdkit.Chem import Draw\n",
        "mols = hiv['Ligand SMILES'][:20]\n",
        "\n",
        "#MolsToGridImage allows to paint a number of molecules at a time\n",
        "Draw.MolsToGridImage(mols, molsPerRow=5, useSVG=True, legends=list(hiv['Ligand SMILES'][:20].values))\n",
        "\n",
        "\"\"\"Generating Molecular Representation in Graph\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install networkx\n",
        "\n",
        "# import library --------------------------------------------------------------\n",
        "from rdkit import Chem\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define the smiles string and covert it into a molecule sturcture ------------\n",
        "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "caffeine_mol = Chem.MolFromSmiles(caffeine_smiles)\n",
        "\n",
        "# define the function for coverting rdkit object to networkx object -----------\n",
        "def mol_to_nx(mol):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        G.add_node(atom.GetIdx(),\n",
        "                   atomic_num=atom.GetAtomicNum(),\n",
        "                   is_aromatic=atom.GetIsAromatic(),\n",
        "                   atom_symbol=atom.GetSymbol())\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        G.add_edge(bond.GetBeginAtomIdx(),\n",
        "                   bond.GetEndAtomIdx(),\n",
        "                   bond_type=bond.GetBondType())\n",
        "\n",
        "    return G\n",
        "\n",
        "# conver rdkit object to networkx object --------------------------------------\n",
        "caffeine_nx = mol_to_nx(caffeine_mol)\n",
        "\n",
        "caffeine_atom = nx.get_node_attributes(caffeine_nx, 'atom_symbol')\n",
        "\n",
        "color_map = {'C': 'cyan',\n",
        "             'O': 'orange',\n",
        "             'N': 'magenta'}\n",
        "\n",
        "caffeine_colors = []\n",
        "for idx in caffeine_nx.nodes():\n",
        "    if (caffeine_nx.nodes[idx]['atom_symbol'] in color_map):\n",
        "        caffeine_colors.append(color_map[caffeine_nx.nodes[idx]['atom_symbol']])\n",
        "    else:\n",
        "        caffeine_colors.append('gray')\n",
        "\n",
        "nx.draw(caffeine_nx,\n",
        "        labels=caffeine_atom,\n",
        "        with_labels = True,\n",
        "        node_color=caffeine_colors,\n",
        "        node_size=800)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# print out the adjacency matrix ----------------------------------------------\n",
        "matrix = np.asmatrix(nx.to_numpy_array(caffeine_nx))#nx.to_numpy_array(caffeine_nx)  #nx.to_numpy_matrix(caffeine_nx)\n",
        "print(matrix)\n",
        "\n",
        "!pip install --upgrade Graph2Vec\n",
        "\n",
        "!pip install karateclub\n",
        "\n",
        "\"\"\"Generating graph embedding for the molecules in the HIV/bindingDB dataset\"\"\"\n",
        "\n",
        "#import rdkit/networkx/graph2vec ---------------------------------------------\n",
        "from rdkit import Chem\n",
        "import networkx as nx\n",
        "#import Graph2Vec\n",
        "from karateclub import Graph2Vec\n",
        "\n",
        "# import numpy/pandas ---------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\">>> read the data file ... \")\n",
        "hiv = pd.read_csv('/content/bindingDB_ic50.csv')    #pd.read_csv('HIV.csv')\n",
        "print(\">>> data shape = \", hiv.shape)\n",
        "print(\">>> data columns = \", hiv.columns, \"\\n\")\n",
        "print(hiv)\n",
        "print()\n",
        "\n",
        "print(\">>> create mol from smiles ... \")\n",
        "hiv['mol'] = hiv['Ligand SMILES'].apply(lambda x: Chem.MolFromSmiles(x))\n",
        "\n",
        "# define the function for coverting rdkit object to networkx object -----------\n",
        "def mol_to_nx(mol):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        G.add_node(atom.GetIdx(),\n",
        "                   atomic_num=atom.GetAtomicNum(),\n",
        "                   is_aromatic=atom.GetIsAromatic(),\n",
        "                   atom_symbol=atom.GetSymbol())\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        G.add_edge(bond.GetBeginAtomIdx(),\n",
        "                   bond.GetEndAtomIdx(),\n",
        "                   bond_type=bond.GetBondType())\n",
        "\n",
        "    return G\n",
        "\n",
        "print(\">>> create nx from mol ... \")\n",
        "hiv['graph'] = hiv['mol'].apply(lambda x: mol_to_nx(x))\n",
        "\n",
        "print(\">>> create graph embedding ... \")\n",
        "model = Graph2Vec()\n",
        "model.fit(hiv['graph'])\n",
        "hiv_graph2vec = model.get_embedding()\n",
        "\n",
        "hiv_graph2vec = pd.DataFrame(hiv_graph2vec)\n",
        "print(\">>> hiv_graph2vec shape = \", hiv_graph2vec.shape)\n",
        "print(hiv_graph2vec)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Conversion of Protein Sequence to Protein Graph\n",
        "\n",
        "Loading in PDB files\n",
        "\"\"\"\n",
        "\n",
        "!pip install proteingraph\n",
        "\n",
        "!pip install biopandas prody\n",
        "\n",
        "import pandas as pd\n",
        "from biopandas.pdb import PandasPdb\n",
        "from prody import parsePDBHeader\n",
        "from typing import Optional\n",
        "\n",
        "def read_pdb_to_dataframe(\n",
        "    pdb_path: Optional[str] = None,\n",
        "    model_index: int = 1,\n",
        "    parse_header: bool = True,\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read a PDB file, and return a Pandas DataFrame containing the atomic coordinates and metadata.\n",
        "\n",
        "    Args:\n",
        "        pdb_path (str, optional): Path to a local PDB file to read. Defaults to None.\n",
        "        model_index (int, optional): Index of the model to extract from the PDB file, in case\n",
        "            it contains multiple models. Defaults to 1.\n",
        "        parse_header (bool, optional): Whether to parse the PDB header and extract metadata.\n",
        "            Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the atomic coordinates and metadata, with one row\n",
        "            per atom\n",
        "    \"\"\"\n",
        "    atomic_df = PandasPdb().read_pdb(pdb_path)\n",
        "    if parse_header:\n",
        "        header = parsePDBHeader(pdb_path)\n",
        "    else:\n",
        "        header = None\n",
        "    atomic_df = atomic_df.get_model(model_index)\n",
        "    if len(atomic_df.df[\"ATOM\"]) == 0:\n",
        "        raise ValueError(f\"No model found for index: {model_index}\")\n",
        "\n",
        "    return pd.concat([atomic_df.df[\"ATOM\"], atomic_df.df[\"HETATM\"]]), header\n",
        "\n",
        "df, df_header = read_pdb_to_dataframe('/content/3cdg.pdb')\n",
        "df.head(10)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "print(df_header.keys())\n",
        "\n",
        "df.loc[df['atom_name']=='CA', ['x_coord', 'y_coord', 'z_coord']]\n",
        "\n",
        "\"\"\"Simple 3D Visualizations — Plotting Atoms\"\"\"\n",
        "\n",
        "!pip install plotly\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_3d(df, x='x_coord', y='y_coord', z='z_coord', color='element_symbol')\n",
        "fig.update_traces(marker_size = 4)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "\"\"\"**Manipulating PDB Dataframes — Building protein graphs**\"\"\"\n",
        "\n",
        "!pip install graphein\n",
        "\n",
        "\"\"\"We first process the dataframe using standard pandas methods. It is important to label each node (atom/residue) with a unique identifier.\"\"\"\n",
        "\n",
        "from graphein.protein.graphs import label_node_id\n",
        "\n",
        "def process_dataframe(df: pd.DataFrame, granularity='CA') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process a DataFrame of protein structure data to reduce ambiguity and simplify analysis.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Handles alternate locations for an atom, defaulting to keep the first one if multiple exist.\n",
        "    2. Assigns a unique node_id to each residue in the DataFrame, using a helper function label_node_id.\n",
        "    3. Filters the DataFrame based on specified granularity (defaults to 'CA' for alpha carbon).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The DataFrame containing protein structure data to process. It is expected to contain columns 'alt_loc' and 'atom_name'.\n",
        "\n",
        "    granularity : str, optional\n",
        "        The level of detail or perspective at which the DataFrame should be analyzed. Defaults to 'CA' (alpha carbon).\n",
        "    \"\"\"\n",
        "    # handle the case of alternative locations,\n",
        "    # if so default to the 1st one = A\n",
        "    if 'alt_loc' in df.columns:\n",
        "      df['alt_loc'] = df['alt_loc'].replace('', 'A')\n",
        "      df = df.loc[(df['alt_loc']=='A')]\n",
        "    df = label_node_id(df, granularity)\n",
        "    df = df.loc[(df['atom_name']==granularity)]\n",
        "    return df\n",
        "\n",
        "process_df = process_dataframe(df)\n",
        "print(process_df.shape)\n",
        "\n",
        "from graphein.protein.graphs import initialise_graph_with_metadata\n",
        "g = initialise_graph_with_metadata(protein_df=process_df, # from above cell\n",
        "                                    raw_pdb_df=df, # Store this for traceability\n",
        "                                    pdb_code = '3cdg', #and again\n",
        "                                    granularity = 'CA' # Store this so we know what kind of graph we have\n",
        "                                    )\n",
        "\n",
        "\"\"\"Add all the nodes to our graph\"\"\"\n",
        "\n",
        "from graphein.protein.graphs import add_nodes_to_graph\n",
        "g = add_nodes_to_graph(g)\n",
        "print(g.nodes)\n",
        "\n",
        "\"\"\"Add Peptide bond\"\"\"\n",
        "\n",
        "import networkx as nx\n",
        "def add_backbone_edges(G: nx.Graph) -> nx.Graph:\n",
        "    # Iterate over every chain\n",
        "    for chain_id in G.graph[\"chain_ids\"]:\n",
        "        # Find chain residues\n",
        "        chain_residues = [\n",
        "            (n, v) for n, v in G.nodes(data=True) if v[\"chain_id\"] == chain_id\n",
        "        ]\n",
        "        # Iterate over every residue in chain\n",
        "        for i, residue in enumerate(chain_residues):\n",
        "            try:\n",
        "                # Checks not at chain terminus\n",
        "                if i == len(chain_residues) - 1:\n",
        "                    continue\n",
        "                # Asserts residues are on the same chain\n",
        "                cond_1 = ( residue[1][\"chain_id\"] == chain_residues[i + 1][1][\"chain_id\"])\n",
        "                # Asserts residue numbers are adjacent\n",
        "                cond_2 = (abs(residue[1][\"residue_number\"] - chain_residues[i + 1][1][\"residue_number\"])== 1)\n",
        "\n",
        "                # If this checks out, we add a peptide bond\n",
        "                if (cond_1) and (cond_2):\n",
        "                    # Adds \"peptide bond\" between current residue and the next\n",
        "                    if G.has_edge(i, i + 1):\n",
        "                        G.edges[i, i + 1][\"kind\"].add('backbone_bond')\n",
        "                    else:\n",
        "                        G.add_edge(residue[0],chain_residues[i + 1][0],kind={'backbone_bond'},)\n",
        "            except IndexError as e:\n",
        "                print(e)\n",
        "    return G\n",
        "\n",
        "g = add_backbone_edges(g)\n",
        "print(len(g.edges()))\n",
        "\n",
        "from graphein.protein.visualisation import plotly_protein_structure_graph\n",
        "\n",
        "p = plotly_protein_structure_graph(\n",
        "    g,\n",
        "    colour_edges_by=\"kind\",\n",
        "    colour_nodes_by=\"seq_position\",\n",
        "    label_node_ids=False,\n",
        "    plot_title=\"3CDG Backbone Protein Graph\",\n",
        "    node_size_multiplier=1,\n",
        ")\n",
        "p.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Working with multi-chain PDB files 6KDE but i have used 4hx1\"\"\"\n",
        "\n",
        "df, df_header = read_pdb_to_dataframe('/content/4hx1.pdb')\n",
        "fig = px.scatter_3d(df, x='x_coord', y='y_coord', z='z_coord', color='chain_id')\n",
        "fig.update_traces(marker_size = 4)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "df = pd.concat((df[df['chain_id']=='A'], df[df['chain_id']=='B']))\n",
        "\n",
        "df['chain_id'].unique()\n",
        "\n",
        "\"\"\"Extracting protein interfaces using Pandas\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_contact_atoms(df1: pd.DataFrame, df2:pd.DataFrame = None , threshold:float = 7., coord_names=['x_coord', 'y_coord', 'z_coord']):\n",
        "    # Extract coordinates from dataframes\n",
        "    coords1 = df1[coord_names].to_numpy()\n",
        "    coords2 = df2[coord_names].to_numpy()\n",
        "\n",
        "    # Compute pairwise distances between atoms\n",
        "    dist_matrix = np.sqrt(((coords1[:, None] - coords2) ** 2).sum(axis=2))\n",
        "\n",
        "    # Create a new dataframe containing pairs of atoms whose distance is below the threshold\n",
        "    pairs = np.argwhere(dist_matrix < threshold)\n",
        "    atoms1, atoms2 = df1.iloc[pairs[:, 0]], df2.iloc[pairs[:, 1]]\n",
        "    atoms1_id = atoms1['chain_id'].map(str) + \":\" + atoms1['residue_name'].map(str) + \":\" + atoms1['residue_number'].map(str)\n",
        "    atoms2_id = atoms2['chain_id'].map(str) + \":\" + atoms2['residue_name'].map(str) + \":\" + atoms2['residue_number'].map(str)\n",
        "    node_pairs = np.vstack((atoms1_id.values, atoms2_id.values)).T\n",
        "    result = pd.concat([df1.iloc[np.unique(pairs[:, 0])], df2.iloc[np.unique(pairs[:, 1])]])\n",
        "\n",
        "    return result, node_pairs\n",
        "\n",
        "\"\"\"In this example we will get all atoms between chains A and B that are 7 Angstrom appart:\"\"\"\n",
        "\n",
        "df_A = df[df['chain_id']=='A']\n",
        "df_B = df[df['chain_id']=='B']\n",
        "contact_atoms, node_pairs = get_contact_atoms(df_A, df_B, 7.)\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create 3D scatter plots for df_A, df_B, and contact_atoms\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add trace for df_A\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        name='Chain A',\n",
        "        x=df_A['x_coord'],\n",
        "        y=df_A['y_coord'],\n",
        "        z=df_A['z_coord'],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color='blue', opacity=0.5),  # Adjust opacity here\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add trace for df_B\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        name='Chain B',\n",
        "        x=df_B['x_coord'],\n",
        "        y=df_B['y_coord'],\n",
        "        z=df_B['z_coord'],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color='red', opacity=0.5),  # Adjust opacity here\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add trace for contact_atoms\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        name='Contact Atoms',\n",
        "        x=contact_atoms['x_coord'],\n",
        "        y=contact_atoms['y_coord'],\n",
        "        z=contact_atoms['z_coord'],\n",
        "        mode='markers',\n",
        "        marker=dict(size=6, color='orange'),  # You can adjust the color and opacity here\n",
        "    )\n",
        ")\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        aspectmode=\"cube\",  # To maintain equal axis scaling\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "!pip install data\n",
        "\n",
        "!pip install dset\n",
        "\n",
        "!pip install tool\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from data.dset_tool import load_raw_dset\n",
        "from models.df_ppi_model_new_fusion_layer import df_ppi_with_fusion_layer\n",
        "from protein_descriptors.extracted_handcrafted_features.read_mat_file import read_mat_to_feat\n",
        "from protein_embedding.prepare import prot_to_token\n",
        "from utils.read_mat_file import loadmat_TEST_feat, loadmat_TEST_seq\n",
        "\n",
        "\n",
        "def get_avelen(inds, dset):\n",
        "    pos_A, pos_B, neg_A, neg_B = dset['seq_pairs']\n",
        "    pos_AB = np.hstack((pos_A, pos_B))\n",
        "    neg_AB = np.hstack((neg_A, neg_B))\n",
        "    prots = np.concatenate((pos_AB, neg_AB), axis=0)\n",
        "    prots = prots.flatten()\n",
        "    prots = prots[inds]\n",
        "    prots = np.unique(prots)\n",
        "    do_dai = [len(seq) for seq in prots]\n",
        "    avelen = int(sum(do_dai) / len(do_dai))\n",
        "    return avelen\n",
        "\n",
        "\n",
        "def load_new_feature(path_to_dir, name_APAACplus_):\n",
        "    def ghep_cap(uni_, pair_):\n",
        "        return uni_.loc[pair_.proteinA], uni_.loc[pair_.proteinB]\n",
        "\n",
        "    def load_positive():\n",
        "        POS_ = pd.read_csv('/content/pairs_pos.csv')\n",
        "        print(POS_)\n",
        "\n",
        "        f1_A, f1_B = ghep_cap(Fvector, POS_)\n",
        "        f2_A, f2_B = ghep_cap(LD, POS_)\n",
        "        f3_A, f3_B = ghep_cap(APAACplus, POS_)\n",
        "        pos_A_ = np.concatenate([f1_A, f3_A, f2_A], axis=1)\n",
        "        pos_B_ = np.concatenate([f1_B, f3_B, f2_B], axis=1)\n",
        "\n",
        "        return pos_A_, pos_B_\n",
        "\n",
        "    def load_negative():\n",
        "        NEG_ = pd.read_csv('/content/pairs_neg.csv')\n",
        "        print(NEG_)\n",
        "\n",
        "        f1_A, f1_B = ghep_cap(Fvector, NEG_)\n",
        "        f2_A, f2_B = ghep_cap(LD, NEG_)\n",
        "        f3_A, f3_B = ghep_cap(APAACplus, NEG_)\n",
        "        neg_A_ = np.concatenate([f1_A, f3_A, f2_A], axis=1)\n",
        "        neg_B_ = np.concatenate([f1_B, f3_B, f2_B], axis=1)\n",
        "\n",
        "        return neg_A_, neg_B_\n",
        "\n",
        "    Fvector = read_mat_to_feat(path_to_dir + '/uni_Fvector_new.mat')\n",
        "    print(Fvector)\n",
        "\n",
        "    LD = read_mat_to_feat(path_to_dir + '/uni_LD.mat')\n",
        "    print(LD)\n",
        "\n",
        "    APAACplus = read_mat_to_feat(path_to_dir + '/' + name_APAACplus_ + '.mat')\n",
        "    print(APAACplus)\n",
        "\n",
        "    # print(\"\\n\", path_to_dir + '/' + name_APAACplus + '.mat')\n",
        "\n",
        "    pos_feat_A, pos_feat_B = load_positive()\n",
        "    neg_feat_A, neg_feat_B = load_negative()\n",
        "\n",
        "    return pos_feat_A, pos_feat_B, neg_feat_A, neg_feat_B, pos_feat_A.shape[1]\n",
        "\n",
        "\n",
        "def prepare_YEAST_token_and_handfeat_new(vocal, W1, fixlen_, dset, name_APAACplus_):\n",
        "    # ----------------------------------------------------------------\n",
        "    # Load hancrafted features\n",
        "    # ----------------------------------------------------------------\n",
        "    p = r\"protein_descriptors/extracted_handcrafted_features/Yeastcore/apaacplus\"\n",
        "    pos_hand_A, pos_hand_B, neg_hand_A, neg_hand_B, handfeat_dim = load_new_feature(p, name_APAACplus_)\n",
        "\n",
        "    hand_A = np.vstack([pos_hand_A, neg_hand_A])\n",
        "    hand_B = np.vstack([pos_hand_B, neg_hand_B])\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Load embedding features\n",
        "    # ----------------------------------------------------------------\n",
        "    pos_seq_A, pos_seq_B, neg_seq_A, neg_seq_B = dset['seq_pairs']\n",
        "    seqs_A = np.concatenate([pos_seq_A, neg_seq_A], axis=0)\n",
        "    seqs_B = np.concatenate([pos_seq_B, neg_seq_B], axis=0)\n",
        "\n",
        "    tokens_A = prot_to_token(vocal, seqs_A, fixlen_)\n",
        "    tokens_B = prot_to_token(vocal, seqs_B, fixlen_)\n",
        "\n",
        "    # print('tokens_A', tokens_A.shape)\n",
        "\n",
        "    return tokens_A, hand_A, tokens_B, hand_B, handfeat_dim\n",
        "\n",
        "\n",
        "def train_all_YEAST(featute_set: tuple, true_label, name_model, name_APAACplus_):\n",
        "    start_time = time.time()\n",
        "    tokens_A, hand_A, tokens_B, hand_B, handdim = prepare_YEAST_token_and_handfeat_new(trained_W1['vocabulary'],\n",
        "                                                                                       trained_W1[\n",
        "                                                                                           'embedding_matrix'],\n",
        "                                                                                       protlen,\n",
        "                                                                                       yeast_dset, name_APAACplus_)\n",
        "    print(hand_A.shape)\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # Shuffle data\n",
        "    # --------------------------------------------------------------\n",
        "    np.random.seed(123456)\n",
        "    indx = np.arange(len(true_label))\n",
        "    np.random.shuffle(indx)\n",
        "    tokens_A, hand_A, tokens_B, hand_B = tokens_A[indx], hand_A[indx], tokens_B[indx], hand_B[indx]\n",
        "    true_label = true_label[indx]\n",
        "\n",
        "    true_label = to_categorical(true_label)\n",
        "    opt = Adam(learning_rate=0.001, decay=0.001)\n",
        "\n",
        "    model_out = df_ppi_with_fusion_layer(protlen, handdim, W1=trained_W1['embedding_matrix'], n_units=1024)\n",
        "\n",
        "    model_out.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    # Train model\n",
        "    model_out.fit([tokens_A, hand_A, tokens_B, hand_B], true_label,\n",
        "                  batch_size=100,\n",
        "                  epochs=64,\n",
        "                  shuffle=False,\n",
        "                  verbose=0)\n",
        "\n",
        "    print(\"Training time: \", time.time() - start_time)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # SAVE\n",
        "    # -------------------------------------------------------------------------\n",
        "    try:\n",
        "        model_out.save(name_model)\n",
        "    except:\n",
        "        new_dir = input(\"SAVE ERROR, ENTER new folder to save:\")\n",
        "        os.mkdir(new_dir)\n",
        "        model_out.save(name_model)\n",
        "\n",
        "    return model_out\n",
        "\n",
        "\n",
        "def load_Xy_test(path_to_dir, testset_name):\n",
        "    Fvector_A, Fvector_B = loadmat_TEST_feat(path_to_dir + '/feat_' + testset_name + '_Fvector_new.mat')\n",
        "    LD_A, LD_B = loadmat_TEST_feat(path_to_dir + '/feat_' + testset_name + '_LD.mat')\n",
        "    APAACplus_A, APAACplus_B = loadmat_TEST_feat(path_to_dir + '/feat_' + testset_name + '_APAACPlus_lg30.mat')\n",
        "\n",
        "    feat_A = np.concatenate([Fvector_A, LD_A, APAACplus_A], axis=1)\n",
        "    feat_B = np.concatenate([Fvector_B, LD_B, APAACplus_B], axis=1)\n",
        "\n",
        "    print(feat_B.shape)\n",
        "    true_label = np.array([1] * len(feat_B))\n",
        "\n",
        "    return feat_A, feat_B, true_label\n",
        "\n",
        "\n",
        "def run_test(testset_name):\n",
        "    path_to_dir = \"protein_descriptors/extracted_handcrafted_features/extracted_TEST\"\n",
        "    X_test_A, X_test_B, y_test = load_Xy_test(path_to_dir, testset_name)\n",
        "\n",
        "    test_seq_A, test_seq_B = loadmat_TEST_seq('data/Testsets/' + testset_name + '.mat')\n",
        "\n",
        "    tokens_A = prot_to_token(trained_W1['vocabulary'], test_seq_A, protlen)\n",
        "    tokens_B = prot_to_token(trained_W1['vocabulary'], test_seq_B, protlen)\n",
        "\n",
        "    y_prob = trained_model.predict([tokens_A, X_test_A, tokens_B, X_test_B])\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # SAVE\n",
        "    # ----------------------------------------------------------\n",
        "    pickle.dump(y_prob, open(\"results/independent_TEST_full_yeastcore_run9/y_prob_on_\" + testset_name + \".pkl\", \"wb\"))\n",
        "\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "    ACC = round(np.sum(y_pred == y_test) / len(y_test), 4)\n",
        "    print(testset_name, '> Accuracy {}'.format(ACC), np.sum(y_pred == y_test), len(y_test))\n",
        "\n",
        "    # return A, B, handfeat_dim\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_W1 = pickle.load(open(\"protein_embedding/doc2vec/d2v_1gram_32dim.pkl\", \"rb\"))\n",
        "    trained_W1['vocabulary']['<unk>'] = len(trained_W1['vocabulary'])\n",
        "    trained_W1['embedding_matrix'] = np.concatenate([trained_W1['embedding_matrix'],\n",
        "                                                     np.zeros((1, trained_W1['embedding_matrix'].shape[1]))], axis=0)\n",
        "\n",
        "    print(\"Embedding\", trained_W1['embedding_matrix'].shape)\n",
        "    print(\"Vocabulary\", len(trained_W1['vocabulary']))\n",
        "\n",
        "    yeast_dset, summary = load_raw_dset(\"data/Yeastcore\")\n",
        "    id_pairs = yeast_dset['id_pairs']\n",
        "    labels = yeast_dset['labels']\n",
        "    print(\"Summary:\", summary)\n",
        "    print(\"Number of pairs:\", len(id_pairs))\n",
        "\n",
        "    protlen = get_avelen(np.arange(len(id_pairs)), yeast_dset)\n",
        "    print(\"Fixed length protein\", protlen)\n",
        "\n",
        "    savedir = \"trained_Yeastcore_d2v_1gram_32dim_run9.h5\"\n",
        "\n",
        "    if not os.path.exists(savedir):\n",
        "        lg = 30\n",
        "        name_APAACplus = 'uni_APAACPlus_lg' + str(lg)\n",
        "        trained_model = train_all_YEAST(id_pairs, labels, savedir, name_APAACplus)\n",
        "    else:\n",
        "        trained_model = load_model(savedir)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # Test on Cross-species\n",
        "    # ---------------------------------------------------\n",
        "    run_test(\"Celeg\")\n",
        "    run_test(\"Ecoli\")\n",
        "    run_test(\"Hpylo\")\n",
        "    run_test(\"Hsapi\")\n",
        "    run_test(\"Mmusc\")\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # Test on PPI network\n",
        "    # ---------------------------------------------------\n",
        "    run_test(\"Onecore\")\n",
        "    run_test(\"Wnt\")\n",
        "    run_test(\"Cancer\")\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import os.path as osp\n",
        "from itertools import product\n",
        "from typing import Callable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch_geometric.data import (\n",
        "    Data,\n",
        "    InMemoryDataset,\n",
        "    download_url,\n",
        "    extract_zip,\n",
        ")\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "\n",
        "class PPI(InMemoryDataset):\n",
        "    r\"\"\"The protein-protein interaction networks from the `\"Predicting\n",
        "    Multicellular Function through Multi-layer Tissue Networks\"\n",
        "    <https://arxiv.org/abs/1707.04638>`_ paper, containing positional gene\n",
        "    sets, motif gene sets and immunological signatures as features (50 in\n",
        "    total) and gene ontology sets as labels (121 in total).\n",
        "\n",
        "    Args:\n",
        "        root (str): Root directory where the dataset should be saved.\n",
        "        split (str, optional): If :obj:`\"train\"`, loads the training dataset.\n",
        "            If :obj:`\"val\"`, loads the validation dataset.\n",
        "            If :obj:`\"test\"`, loads the test dataset. (default: :obj:`\"train\"`)\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "        pre_transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.Data` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            being saved to disk. (default: :obj:`None`)\n",
        "        pre_filter (callable, optional): A function that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
        "            value, indicating whether the data object should be included in the\n",
        "            final dataset. (default: :obj:`None`)\n",
        "        force_reload (bool, optional): Whether to re-process the dataset.\n",
        "            (default: :obj:`False`)\n",
        "\n",
        "    **STATS:**\n",
        "\n",
        "    .. list-table::\n",
        "        :widths: 10 10 10 10 10\n",
        "        :header-rows: 1\n",
        "\n",
        "        * - #graphs\n",
        "          - #nodes\n",
        "          - #edges\n",
        "          - #features\n",
        "          - #tasks\n",
        "        * - 20\n",
        "          - ~2,245.3\n",
        "          - ~61,318.4\n",
        "          - 50\n",
        "          - 121\n",
        "    \"\"\"\n",
        "\n",
        "    url = '/content/diamond_ppi.txt' #'https://data.dgl.ai/dataset/ppi.zip'\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        split: str = 'train',\n",
        "        transform: Optional[Callable] = None,\n",
        "        pre_transform: Optional[Callable] = None,\n",
        "        pre_filter: Optional[Callable] = None,\n",
        "        force_reload: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        assert split in ['train', 'val', 'test']\n",
        "\n",
        "        super().__init__(root, transform, pre_transform, pre_filter,\n",
        "                         force_reload=force_reload)\n",
        "\n",
        "        if split == 'train':\n",
        "            self.load(self.processed_paths[0])\n",
        "        elif split == 'val':\n",
        "            self.load(self.processed_paths[1])\n",
        "        elif split == 'test':\n",
        "            self.load(self.processed_paths[2])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> List[str]:\n",
        "        splits = ['train', 'valid', 'test']\n",
        "        files = ['feats.npy', 'graph_id.npy', 'graph.json', 'labels.npy']\n",
        "        return [f'{split}_{name}' for split, name in product(splits, files)]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> List[str]:\n",
        "        return ['train.pt', 'val.pt', 'test.pt']\n",
        "\n",
        "    def download(self) -> None:\n",
        "        path = download_url(self.url, self.root)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        os.unlink(path)\n",
        "\n",
        "    def process(self) -> None:\n",
        "        import networkx as nx\n",
        "        from networkx.readwrite import json_graph\n",
        "\n",
        "        for s, split in enumerate(['train', 'valid', 'test']):\n",
        "            path = osp.join(self.raw_dir, f'{split}_graph.json')\n",
        "            with open(path, 'r') as f:\n",
        "                G = nx.DiGraph(json_graph.node_link_graph(json.load(f)))\n",
        "\n",
        "            x = np.load(osp.join(self.raw_dir, f'{split}_feats.npy'))\n",
        "            x = torch.from_numpy(x).to(torch.float)\n",
        "\n",
        "            y = np.load(osp.join(self.raw_dir, f'{split}_labels.npy'))\n",
        "            y = torch.from_numpy(y).to(torch.float)\n",
        "\n",
        "            data_list = []\n",
        "            path = osp.join(self.raw_dir, f'{split}_graph_id.npy')\n",
        "            idx = torch.from_numpy(np.load(path)).to(torch.long)\n",
        "            idx = idx - idx.min()\n",
        "\n",
        "            for i in range(int(idx.max()) + 1):\n",
        "                mask = idx == i\n",
        "\n",
        "                G_s = G.subgraph(\n",
        "                    mask.nonzero(as_tuple=False).view(-1).tolist())\n",
        "                edge_index = torch.tensor(list(G_s.edges)).t().contiguous()\n",
        "                edge_index = edge_index - edge_index.min()\n",
        "                edge_index, _ = remove_self_loops(edge_index)\n",
        "\n",
        "                data = Data(edge_index=edge_index, x=x[mask], y=y[mask])\n",
        "\n",
        "                if self.pre_filter is not None and not self.pre_filter(data):\n",
        "                    continue\n",
        "\n",
        "                if self.pre_transform is not None:\n",
        "                    data = self.pre_transform(data)\n",
        "\n",
        "                data_list.append(data)\n",
        "            self.save(data_list, self.processed_paths[s])\n",
        "\n",
        "\"\"\"**SGPPI: structure-aware prediction of protein-protein interactions in rigorous conditions with graph convolutional network**\"\"\"\n",
        "\n",
        "!git clone https://github.com/emersON106/SGPPI.git\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %cd SGPPI\n",
        "# %ls\n",
        "\n",
        "!pip install dgl\n",
        "\n",
        "!pip install torch==1.5.0\n",
        "\n",
        "!pip install scipy==1.5.2\n",
        "\n",
        "!pip install scikit-learn==0.24.2\n",
        "\n",
        "!pip install dgl==0.7.2\n",
        "\n",
        "!pip install numpy==1.19.1\n",
        "\n",
        "!python feature_extract.py -i Q96B49 -o protein_features.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"**Building a Solubility Molecule Prediction Model using Graph Neural Networks**\"\"\"\n",
        "\n",
        "!pip install pysmiles\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from pysmiles import read_smiles\n",
        "import pandas as pd\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU, CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, GCNConv\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, degree\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "logging.getLogger('pysmiles').setLevel(logging.CRITICAL)  # Anything higher than warning\n",
        "\n",
        "df = pd.read_csv('/content/bindingDB_ic50.csv') #read dataset (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OVHAW8)\n",
        "X_smiles = list(df['Ligand SMILES']) #get smiles strings from file\n",
        "#Y = np.asarray(df['Solubility']) #get solubility values from file\n",
        "Y = np.asarray(df['IC50 (nM)']) #get solubility values from file\n",
        "elements = ['K', 'Y', 'V', 'Sm', 'Dy', 'In', 'Lu', 'Hg', 'Co', 'Mg',    #list of all elements in the dataset\n",
        "            'Cu', 'Rh', 'Hf', 'O', 'As', 'Ge', 'Au', 'Mo', 'Br', 'Ce',\n",
        "            'Zr', 'Ag', 'Ba', 'N', 'Cr', 'Sr', 'Fe', 'Gd', 'I', 'Al',\n",
        "            'B', 'Se', 'Pr', 'Te', 'Cd', 'Pd', 'Si', 'Zn', 'Pb', 'Sn',\n",
        "            'Cl', 'Mn', 'Cs', 'Na', 'S', 'Ti', 'Ni', 'Ru', 'Ca', 'Nd',\n",
        "            'W', 'H', 'Li', 'Sb', 'Bi', 'La', 'Pt', 'Nb', 'P', 'F', 'C']\n",
        "\n",
        "#convert element to a one-hot vector of dimension len(elements)\n",
        "def element_to_onehot(element):\n",
        "    out = []\n",
        "    for i in range(0, len(element)):\n",
        "        v = np.zeros(len(elements))\n",
        "        v[elements.index(element[i])] = 1.0\n",
        "        out.append(v)\n",
        "    return np.asarray(out)\n",
        "\n",
        "#convert solubility value to one-hot class vector\n",
        "def val_to_class(val):\n",
        "    if val < -3.65: #insoluble\n",
        "        return [1, 0, 0]\n",
        "    elif val < -1.69: #slightly soluble\n",
        "        return [0, 1, 0]\n",
        "    else: #soluble\n",
        "        return [0, 0, 1]\n",
        "\n",
        "#process SMILES strings into graphs\n",
        "nodes = []\n",
        "edge_index = []\n",
        "for smiles in tqdm(X_smiles):\n",
        "    try:\n",
        "        G = read_smiles(smiles, explicit_hydrogen=True)\n",
        "        feature = element_to_onehot(np.asarray(G.nodes(data='element'))[:, 1])\n",
        "        edges = np.asarray(G.edges)\n",
        "        index = np.asarray([edges[:,0], edges[:,1]]) #reshape indices into shape [2, -1]\n",
        "        nodes.append(feature)\n",
        "        edge_index.append(index)\n",
        "        #print(\"Nodes of SMILES String\",nodes)\n",
        "        #print(\"Edges:\",edge_index)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "#Generate Data objects\n",
        "data = list()\n",
        "\n",
        "#process graphs into torch_geometric Data objects\n",
        "for i in tqdm(range(0, len(nodes))):\n",
        "    x = torch.tensor(nodes[i], dtype=torch.float) #convert node features into torch tensor\n",
        "    edges = torch.tensor(edge_index[i], dtype=torch.long) #convert edge index into torch tensor\n",
        "    y = torch.tensor([val_to_class(Y[i])], dtype=torch.float) #change shape of label and convert to tensor\n",
        "    data.append(Data(x=x,edge_index=edges, y=y)) #add the Data object to the list of data\n",
        "random.shuffle(data)\n",
        "train = data[:int(len(data)*0.8)] #train set\n",
        "test = data[int(len(data)*0.8):] #val set\n",
        "train = data\n",
        "\n",
        "#define the message passing network\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(61, 32)\n",
        "        self.conv2 = GCNConv(32, 32)\n",
        "        self.conv3 = GCNConv(32, 32)\n",
        "        self.conv4 = GCNConv(32, 32)\n",
        "        self.lin1 = Linear(32, 16)\n",
        "        self.lin2 = Linear(16, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index= data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = torch.sum(x, dim=0)\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.lin2(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "#set up device and create model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #use CUDA if available\n",
        "model = Net().to(device) #create network and send to the device memory\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4) #use Adam optimizer\n",
        "CSE = CrossEntropyLoss() #define loss\n",
        "\n",
        "#train model\n",
        "model.train() #set model to training mode\n",
        "for epoch in range(2): #run for epochs of training\n",
        "    sum_loss = 0 #used to compute average loss in an epoch\n",
        "    num_correct = 0\n",
        "    random.shuffle(train) #shuffle the training data each epoch\n",
        "    for d in tqdm(train): #go over each training point\n",
        "        data = d.to(device) #send data to device\n",
        "        optimizer.zero_grad() #zero gradients\n",
        "        out = model(data) #evaluate data point\n",
        "        if torch.argmax(out) == torch.argmax(data.y): #if prediction is correct, increment counter for accuracy calculation\n",
        "            num_correct += 1\n",
        "        loss = CSE(torch.reshape(out, [1, 3]), torch.reshape(torch.argmax(data.y),[1])) #compute mean squared error loss\n",
        "        sum_loss += float(loss) #add loss value to aggregate loss\n",
        "        loss.backward() #compute gradients\n",
        "        optimizer.step() #apply optimization\n",
        "    print('Epoch: {:03d}, Average loss: {:.5f}, Accuracy: {:.5f}'.format(epoch, sum_loss/len(train), num_correct/len(train)))\n",
        "\n",
        "#test the model and display a histogram of the outputs\n",
        "num_correct = 0\n",
        "model.eval()\n",
        "predictions = list()\n",
        "for t in tqdm(test):\n",
        "    d = t.to(device)\n",
        "    out = model(d)\n",
        "    if torch.argmax(out) == torch.argmax(d.y): #if prediction is correct, increment counter for accuracy calculation\n",
        "            num_correct += 1\n",
        "    predictions.append(torch.argmax(out).item())\n",
        "\n",
        "print(\"Test accuracy: \" + str(num_correct/len(test)))\n",
        "plt.hist(predictions, bins=3)\n",
        "\n",
        "#test SMILES string\n",
        "def evaluate_smiles(smiles_string):\n",
        "    classes = ['insoluble', 'slightly soluble', 'soluble']\n",
        "    G = read_smiles(smiles_string, explicit_hydrogen=True) #decode smiles string\n",
        "    feature = element_to_onehot(np.asarray(G.nodes(data='element'))[:, 1]) #convert element to one-hot vector\n",
        "    edges = np.asarray(G.edges) #get edge array\n",
        "    index = np.asarray([edges[:,0], edges[:,1]]) #reformat edge array to torch geometric suitable format\n",
        "    d = Data(x=torch.tensor(feature, dtype=torch.float),edge_index=torch.tensor(index, dtype=torch.long)) #create torch gemoetry Data object\n",
        "    data = d.to(device) #send data to device memory\n",
        "    model.eval() #set model to evaluate mode\n",
        "    print(classes[torch.argmax(torch.softmax(model(data), dim=0)).item()]) #evaluate the test data\n",
        "\n",
        "evaluate_smiles('C(C(C1C(=C(C(=O)O1)O)O)O)O') #test out the model on Vitamin C\n",
        "\n",
        "evaluate_smiles('CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(C)c12') #test out the model on\n",
        "\n",
        "evaluate_smiles('CC(C)c1ccccc1-n1c2cc(c(Cl)cc2c(nc1=O)N1CCN(CC1)C(=O)C=C)-c1c(C)ccc2[nH]ncc12')\n",
        "\n",
        "evaluate_smiles('C[C@@H]1CN2[C@H](CN1C(=O)C=C)C(=O)N(C)c1cnc3c(F)c(c(Cl)cc3c21)-c1c(O)cccc1F')\n",
        "\n",
        "evaluate_smiles('CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2cccc(c12)C(F)(F)F')\n",
        "\n",
        "evaluate_smiles('CN1CCC[C@H]1COc1nc2CN(CCc2c(n1)N1CCN([C@@H](CC#N)C1)C(=O)C=C)c1cccc2ccccc12')\n",
        "\n",
        "!pip install rdkit\n",
        "\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "\"\"\"**Preprocessing Molecule Data**\"\"\"\n",
        "\n",
        "def smiles_to_molecule(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        mol = Chem.AddHs(mol)\n",
        "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
        "        return mol\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "df1 = pd.read_csv('/content/curated-solubility-dataset.csv')\n",
        "\n",
        "from rdkit import RDLogger\n",
        "\n",
        "lg = RDLogger.logger()\n",
        "lg.setLevel(RDLogger.CRITICAL)\n",
        "df1['Molecule'] = df1['SMILES'].apply(smiles_to_molecule)\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "molecule = df1['Molecule'].iloc[0]\n",
        "Chem.Draw.MolToImage(molecule)\n",
        "\n",
        "#Feature Engineering\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import pandas as pd\n",
        "\n",
        "# Function to generate edge_index and edge_attr for a molecule\n",
        "def generate_edge_information(molecule):\n",
        "    if molecule is not None and molecule.GetNumAtoms() > 0:\n",
        "        num_atoms = molecule.GetNumAtoms()\n",
        "        edge_index = []  # List to store edge connections\n",
        "        edge_attr = []   # List to store edge attributes (e.g., bond types)\n",
        "\n",
        "        for bond in molecule.GetBonds():\n",
        "            start_idx = bond.GetBeginAtomIdx()\n",
        "            end_idx = bond.GetEndAtomIdx()\n",
        "            bond_type = bond.GetBondTypeAsDouble()  # You can customize this part based on your needs\n",
        "\n",
        "            # Add edge connection\n",
        "            edge_index.append((start_idx, end_idx))\n",
        "            edge_attr.append(bond_type)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "    return None, None\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df1[['edge_index', 'edge_attr']] = df1['Molecule'].apply(generate_edge_information).apply(pd.Series)\n",
        "\n",
        "# Now, your 'train_data' DataFrame contains 'edge_index' and 'edge_attr' columns.\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "\n",
        "# Assuming your DataFrame is named df\n",
        "\n",
        "# Define a function to convert row data to PyTorch Geometric Data object\n",
        "def row_to_graph(row):\n",
        "    x = row['Molecule']  # Assuming 'Molecule' column contains the RDKit molecule objects\n",
        "    edge_index = torch.tensor(row['edge_index'], dtype=torch.long)\n",
        "    edge_attr = torch.tensor(row['edge_attr'], dtype=torch.float)\n",
        "\n",
        "    # You may need to convert other features to tensors and add them to x if necessary\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=row['Solubility'])\n",
        "\n",
        "# Create the \"Graph\" column\n",
        "df1['Graph'] = df1.apply(row_to_graph, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "def calculate_molecular_weight(molecule):\n",
        "    if molecule is not None:\n",
        "        mw = Descriptors.MolWt(molecule)\n",
        "        return mw\n",
        "    return None\n",
        "\n",
        "df1['MolWt'] = df1['Molecule'].apply(calculate_molecular_weight)\n",
        "\n",
        "# Define a function to check the presence of specific functional groups\n",
        "def has_functional_group(mol, functional_group):\n",
        "    if mol is not None:\n",
        "        return mol.HasSubstructMatch(Chem.MolFromSmiles(functional_group))\n",
        "    return None\n",
        "\n",
        "# Example: Check for the presence of a hydroxyl group\n",
        "df1['Group'] = df1['Molecule'].apply(lambda mol: has_functional_group(mol, 'O'))\n",
        "\n",
        "\"\"\"**Graph-Based Representation**\"\"\"\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_graph(row):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Extract edge information and add edges\n",
        "    edge_index = row['edge_index']\n",
        "    edge_attr = row['edge_attr']\n",
        "\n",
        "    for edge, attr in zip(edge_index, edge_attr):\n",
        "        src, tgt = edge\n",
        "        G.add_edge(src, tgt, weight=attr)\n",
        "\n",
        "    # Define the layout and customize node/edge parameters\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.5)  # Adjust 'k' for node separation\n",
        "    labels = nx.get_edge_attributes(G, 'weight')\n",
        "\n",
        "    # Customize the node size, node color, and edge width\n",
        "    node_size = 300\n",
        "    node_color = 'skyblue'\n",
        "    edge_width = 2\n",
        "\n",
        "    # Draw the graph with customization\n",
        "    nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=node_color, font_size=10)\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
        "    edges = nx.draw_networkx_edges(G, pos, width=edge_width)\n",
        "\n",
        "    # Show the graph\n",
        "    plt.title(\"Graph Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have the 'edge_index' and 'edge_attr' in your DataFrame\n",
        "visualize_graph(df1.iloc[0])\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Replace 'df' with your DataFrame containing the SMILES strings\n",
        "# The 'SMILES' column should contain your SMILES strings.\n",
        "smiles_list = df1['SMILES']\n",
        "\n",
        "# Convert SMILES strings to RDKit Mol objects\n",
        "mol_list = [Chem.MolFromSmiles(smiles) for smiles in smiles_list]\n",
        "\n",
        "# Draw and visualize the chemical structures\n",
        "img = Draw.MolsToGridImage(mol_list, molsPerRow=3, subImgSize=(200, 200))\n",
        "\n",
        "# Display the image in Colab\n",
        "display(img)\n",
        "\n",
        "\"\"\"***Data Splitting***\"\"\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'train_data' is your DataFrame\n",
        "train_set, test_set = train_test_split(df1, test_size=200, random_state=42, shuffle=True) #stratify=row['Solubility']\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Training set shape:\", train_set.shape)\n",
        "print(\"Testing set shape:\", test_set.shape)\n",
        "\n",
        "\"\"\"**Preprocessing the train and test data and Creating Pytorch Object**\"\"\"\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'edge_index' and 'edge_attr' are lists in your DataFrame\n",
        "train_set['edge_index'] = train_set['edge_index'].apply(lambda x: torch.tensor(x).t().contiguous())\n",
        "train_set['edge_attr'] = train_set['edge_attr'].apply(lambda x: torch.tensor(x, dtype=torch.float))\n",
        "\n",
        "# Assuming node features are PyTorch tensors\n",
        "node_features = torch.tensor(train_set[['MolWt', 'NumHAcceptors', 'NumHDonors', 'MolLogP']].to_numpy(), dtype=torch.float)\n",
        "\n",
        "# Assuming SENOLYTIC is your target variable\n",
        "target = torch.tensor(train_set['Solubility'].to_numpy(), dtype=torch.long)\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = list(zip(train_set['edge_index'], train_set['edge_attr']))\n",
        "data = [(x[0].t().contiguous(), x[1]) for x in data]\n",
        "data = Data(x=node_features, edge_index=data[0][0], edge_attr=data[0][1], y=target)\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'edge_index' and 'edge_attr' are lists in your DataFrame\n",
        "test_set['edge_index'] = test_set['edge_index'].apply(lambda x: torch.tensor(x).t().contiguous())\n",
        "test_set['edge_attr'] = test_set['edge_attr'].apply(lambda x: torch.tensor(x, dtype=torch.float))\n",
        "\n",
        "\n",
        "# Assuming node features are PyTorch tensors\n",
        "test_node_features = torch.tensor(test_set[['MolWt', 'NumHAcceptors', 'NumHDonors', 'MolLogP']].to_numpy(), dtype=torch.float)\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data_test = list(zip(test_set['edge_index'], test_set['edge_attr']))\n",
        "data_test = [(x[0].t().contiguous(), x[1]) for x in data_test]\n",
        "dataset_test = Data(x=test_node_features, edge_index=data_test[0][0], edge_attr=data_test[0][1])\n",
        "\n",
        "\"\"\"# ***Model Architecture***\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from torch_geometric.nn import GCNConv, global_add_pool\n",
        "import torch.nn.functional as F\n",
        "def add_self_loops(edge_index, edge_attr, fill_value, num_nodes):\n",
        "    loop_index = torch.arange(0, num_nodes, dtype=torch.long)\n",
        "    loop_index = loop_index.view(-1, 1).expand(-1, 2)\n",
        "\n",
        "    print(\"Edge Index Sizes:\", edge_index.size())\n",
        "    print(\"Edge Attribute Sizes:\", edge_attr.size())\n",
        "    print(\"Loop Index Sizes:\", loop_index.size())\n",
        "    num_attr=edge_attr.shape[0]\n",
        "    # Ensure both edge_index and loop_index are contiguous\n",
        "    if not edge_index.is_contiguous():\n",
        "        edge_index = edge_index.contiguous()\n",
        "    if not loop_index.is_contiguous():\n",
        "        loop_index = loop_index.contiguous()\n",
        "\n",
        "    # Check the size after making them contiguous\n",
        "    print(\"Edge Index Sizes (contiguous):\", edge_index.size())\n",
        "    print(\"Loop Index Sizes (contiguous):\", loop_index.size())\n",
        "\n",
        "    loop_attr = torch.full((loop_index.size(1),), fill_value, dtype=edge_attr.dtype)\n",
        "\n",
        "\n",
        "\n",
        "    # Concatenate along dimension 0\n",
        "    edge_index = torch.cat([edge_index, loop_index], dim=0).transpose(-2,1)\n",
        "    edge_attr = torch.cat([edge_attr, loop_attr], dim=0)\n",
        "    num_edges=edge_index.shape[1]\n",
        "\n",
        "    edge_attr = edge_attr.repeat(num_edges // num_attr + 1)[:num_edges]\n",
        "\n",
        "    print(edge_attr.shape)\n",
        "    print(loop_index.shape)\n",
        "\n",
        "    return edge_index, edge_attr\n",
        "\n",
        "\n",
        "\n",
        "# Define your GNN model\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim,hidden_dim,hidden_dim)\n",
        "        self.fc1=nn.Linear(hidden_dim,32)\n",
        "        self.fc2=nn.Linear(32,num_classes)\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        # Add self-loops to the graph only once\n",
        "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr, fill_value=1, num_nodes=x.size(0))\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x).squeeze(1)\n",
        "        # Global pooling to get a graph-level representation\n",
        "        #x = global_add_pool(x, data.batch)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"**Instantiating our GNN Model**\"\"\"\n",
        "\n",
        "num_node_features = 4  # Update with the actual number of node features\n",
        "hidden_dim = 64  # You can adjust this based on your task\n",
        "num_classes = 2  # Assuming binary classification, update as needed\n",
        "\n",
        "model = GNN(num_node_features, hidden_dim, num_classes)\n",
        "\n",
        "#Defining Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Creating Train Loader\n",
        "train_loader = DataLoader([data], batch_size=2372, shuffle=True)\n",
        "\n",
        "#Train the model\n",
        "# Training loop\n",
        "num_epochs = 100 # You can adjust this based on your task\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
        "\n",
        "#Model Evaluation\n",
        "# Assuming 'model' is your pre-trained GNN model\n",
        "pred=[]\n",
        "model.eval()\n",
        "\n",
        "#\n",
        "with torch.no_grad():\n",
        "  for data_ in test_loader:\n",
        "\n",
        "    test_predictions = model(data_)\n",
        "    pred.append(test_predictions)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Convert probabilities to predicted classes\n",
        "predicted_classes = np.argmax(output.numpy(), axis=1)\n",
        "\n",
        "# True classes from your test set\n",
        "true_classes = test_set['SENOLYTIC'].to_numpy()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_classes, predicted_classes)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(true_classes, predicted_classes)\n",
        "recall = recall_score(true_classes, predicted_classes)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install rdkit-pypi\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "#Load Dataset\n",
        "import rdkit\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "\n",
        "# Load the ESOL dataset\n",
        "data = MoleculeNet(root=\".\", name=\"hiv\")\n",
        "data\n",
        "\n",
        "#Investigating the datset\n",
        "print(\"Dataset type: \", type(data))\n",
        "print(\"Dataset features: \", data.num_features)\n",
        "print(\"Dataset target: \", data.num_classes)\n",
        "print(\"Dataset length: \", data.len)\n",
        "print(\"Dataset sample: \", data[0])\n",
        "print(\"Sample  nodes: \", data[0].num_nodes)\n",
        "print(\"Sample  edges: \", data[0].num_edges)\n",
        "\n",
        "# Investiagte the features of the node of graph\n",
        "data[0].x\n",
        "\n",
        "# Investigating the edges in sparse COO format\n",
        "# Shape [2, num_edges]\n",
        "data[0].edge_index.t()\n",
        "\n",
        "# See the target value of data[0]\n",
        "data[0].y\n",
        "\n",
        "#Converting SMILES to RDKit molecules — Visualizing molecules\n",
        "data[0][\"smiles\"]\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "molecule = Chem.MolFromSmiles(data[0][\"smiles\"])\n",
        "molecule\n",
        "\n",
        "#Implementation of GNN\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "embedding_size = 64\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
        "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
        "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
        "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
        "\n",
        "        # Output layer\n",
        "        self.out = Linear(embedding_size*2, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "\n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index),\n",
        "                            gap(hidden, batch_index)], dim=1)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "model = GCN()\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "#Training of GNN\n",
        "from torch_geometric.data import DataLoader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Root mean squared error\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
        "\n",
        "# Use GPU for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Wrap data in a data loader\n",
        "data_size = len(data)\n",
        "NUM_GRAPHS_PER_BATCH = 64\n",
        "loader = DataLoader(data[:int(data_size * 0.8)],\n",
        "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "test_loader = DataLoader(data[int(data_size * 0.8):],\n",
        "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "\n",
        "def train(data):\n",
        "    # Enumerate over the data\n",
        "    for batch in loader:\n",
        "      # Use GPU\n",
        "      batch.to(device)\n",
        "      # Reset gradients\n",
        "      optimizer.zero_grad()\n",
        "      # Passing the node features and the connection info\n",
        "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch)\n",
        "      # Calculating the loss and gradients\n",
        "      loss = loss_fn(pred, batch.y)\n",
        "      loss.backward()\n",
        "      # Update using the gradients\n",
        "      optimizer.step()\n",
        "    return loss, embedding\n",
        "\n",
        "print(\"Starting training...\")\n",
        "losses = []\n",
        "for epoch in range(1000):\n",
        "    loss, h = train(data)\n",
        "    losses.append(loss)\n",
        "    if epoch % 100 == 0:\n",
        "      print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
        "\n",
        "# Visualize learning (training loss) in Lineplot\n",
        "import seaborn as sns\n",
        "losses_float = [float(loss.cpu().detach().numpy()) for loss in losses]\n",
        "loss_indices = [i for i,l in enumerate(losses_float)]\n",
        "\"\"\"plt = sns.lineplot(loss_indices, losses_float)\n",
        "plt\"\"\"\n",
        "plt = sns.lineplot(x=loss_indices, y=losses_float)\n",
        "plt.set_title(\"Loss vs. Index\")\n",
        "plt.set_xlabel(\"Index\")\n",
        "plt.set_ylabel(\"Loss\")\n",
        "\n",
        "#prediction of test data\n",
        "import pandas as pd\n",
        "\n",
        "# Analyze the results for one batch\n",
        "test_batch = next(iter(test_loader))\n",
        "with torch.no_grad():\n",
        "    test_batch.to(device)\n",
        "    pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch)\n",
        "    df = pd.DataFrame()\n",
        "    df[\"y_real\"] = test_batch.y.tolist()\n",
        "    df[\"y_pred\"] = pred.tolist()\n",
        "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
        "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
        "df\n",
        "\n",
        "#Visualize in Scatterplot\n",
        "plt = sns.scatterplot(data=df, x=\"y_real\", y=\"y_pred\")\n",
        "plt.set(xlim=(-7, 2))\n",
        "plt.set(ylim=(-7, 2))\n",
        "plt\n",
        "\n",
        "\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "dataset = 'Cora'\n",
        "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
        "dataset = Planetoid(path, dataset)\n",
        "data = dataset[0]\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")\n",
        "node_index = 10\n",
        "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
        "print(f'Generated explanations in {explanation.available_explanations}')\n",
        "\n",
        "path = 'feature_importance.png'\n",
        "explanation.visualize_feature_importance(path, top_k=10)\n",
        "print(f\"Feature importance plot has been saved to '{path}'\")\n",
        "\n",
        "path = 'subgraph.pdf'\n",
        "explanation.visualize_graph(path)\n",
        "print(f\"Subgraph visualization plot has been saved to '{path}'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AN1rcwCGJv8h",
        "outputId": "fd01ad47-4c61-47b2-dad4-f34da4ca6977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting Chem\n",
            "  Downloading chem-1.3.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from Chem) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from Chem) (1.14.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from Chem) (3.9.1)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.11/dist-packages (from Chem) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->Chem) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->Chem) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->Chem) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->Chem) (4.67.1)\n",
            "Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chem-1.3.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: rdkit, Chem\n",
            "Successfully installed Chem-1.3.0 rdkit-2024.9.6\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric-2.6.1\n",
            "Collecting DataLoader\n",
            "  Downloading dataloader-2.0.tar.gz (9.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: DataLoader\n",
            "  Building wheel for DataLoader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DataLoader: filename=dataloader-2.0-py3-none-any.whl size=10084 sha256=45beb3cb00ff47a6b61e5bd6cdb56b2aaf822ef5bd58e29f4fc3a16ddd249b1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/b4/28/bf711e4f9bf69d9fc21b1c017d9b63eb13c3a6f35308088a0b\n",
            "Successfully built DataLoader\n",
            "Installing collected packages: DataLoader\n",
            "Successfully installed DataLoader-2.0\n",
            "Collecting Data\n",
            "  Downloading data-0.4.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from Data) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from Data) (4.4.2)\n",
            "Collecting funcsigs (from Data)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: Data\n",
            "  Building wheel for Data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Data: filename=data-0.4-py3-none-any.whl size=7227 sha256=32daad6ea8b4a4e23777a7e09ee8641717a6f535f2960624eaa9a8b1e6c934be\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/d3/10/d5fe9bc9dcb197ea289baccca92a25f2f95135235a92ca1b11\n",
            "Successfully built Data\n",
            "Installing collected packages: funcsigs, Data\n",
            "Successfully installed Data-0.4 funcsigs-1.0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06da3b69549b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw1B2R_WCU7X"
      },
      "source": [
        "Welcome to the first practical work of the week! In this practical, we will learn about the programming language Python as well as NumPy and Matplotlib, two fundamental tools for data science and machine learning in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyLSwlxnJqXX"
      },
      "source": [
        "# Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ltwm91eJyQM"
      },
      "source": [
        "Python is one of the most popular programming languages for machine learning, both in academia and in industry. As such, it is essential to learn this language for anyone interested in machine learning. In this section, we will review Python basics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeGAvLq1ALJ4",
        "outputId": "775c9da1-aa9c-43df-d6d4-500f1dda2d63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "s1kp5Zv0JBSx",
        "outputId": "5c3d203f-046d-4555-d398-36e91dbe685c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKvP6jiMZ9H"
      },
      "source": [
        "## Arithmetic operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDjs0-7YQ80h"
      },
      "source": [
        "Python supports the usual arithmetic operators: + (addition), * (multiplication), / (division), ** (power), // (integer division)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhcbBQUiStHG"
      },
      "source": [
        "## Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkPn1IjNTCxA"
      },
      "source": [
        "Lists are a container type for ordered sequences of elements. Lists can be initialized empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrnV1ySAPtHp"
      },
      "outputs": [],
      "source": [
        "my_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwRqyYI9XnPK"
      },
      "source": [
        "or with some initial elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq5YTJ1JXpOX"
      },
      "outputs": [],
      "source": [
        "my_list = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk2WmojJXyyz"
      },
      "source": [
        "Lists have a dynamic size and elements can be added (appended) to them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "QFTNqiYiXxAh",
        "outputId": "720d3340-7df7-49b8-c920-964d2e350af5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_list.append(4)\n",
        "my_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUnJuqQ2Yhzw"
      },
      "source": [
        "We can access individual elements of a list (indexing starts from 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pyFxyZPVYpG_",
        "outputId": "7efb0bc1-25fb-4849-8d3a-cfa62b94a175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_list[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMrIDYsdgMP"
      },
      "source": [
        "We can access \"slices\" of a list using `my_list[i:j]` where `i` is the start of the slice (again, indexing starts from 0) and `j` the end of the slice. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ichf9p0gd7tJ",
        "outputId": "5f4ec085-431e-4e16-aab1-8708f2c931d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_list[1:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMbzH4tzQ9rI"
      },
      "source": [
        "Omitting the second index means that the slice shoud run until the end of the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "O7wCthKnREKV",
        "outputId": "5a04ea59-7a54-4b09-a8ee-3cd6171e8c7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 3, 4]"
            ]
          },
          "execution_count": 64,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_list[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Aeu7PUebrK"
      },
      "source": [
        "We can check if an element is in the list using `in`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "a_P5NCi-efvb",
        "outputId": "5012122e-f02c-4474-ec51-6ef8519e5733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "5 in my_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LypIsP5gkl10"
      },
      "source": [
        "The length of a list can be obtained using the `len` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ac0FMsaKkrWc",
        "outputId": "0a7418d2-67a1-419c-e12b-e6ffb3b4b66d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(my_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c3RLStf7G2I"
      },
      "source": [
        "## Strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm6hZhgz7KhI"
      },
      "source": [
        "Strings are used to store text. They can delimited using either single quotes or double quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCma6Oj_7T8n"
      },
      "outputs": [],
      "source": [
        "string1 = \"some text\"\n",
        "string2 = 'some other text'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irr4xuWu7Znu"
      },
      "source": [
        "Strings behave similarly to lists. As such we can access individual elements in exactly the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "26_POhLO7iM3",
        "outputId": "fa06d3b9-c683-4b05-bedc-36bc43f57bd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'e'"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string1[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_UD0JV7oPw"
      },
      "source": [
        "and similarly for slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dcZFcLqQ7qCe",
        "outputId": "aba6c437-ade3-49da-9e6a-9383eab01fa9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'text'"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string1[5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQ_CIiu76YG"
      },
      "source": [
        "String concatenation is performed using the `+` operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mxqNMKCY79_W",
        "outputId": "e5695c9e-0703-49b3-9608-b57ca8375ba9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'some text some other text'"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string1 + \" \" + string2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lox2GZCMdIB"
      },
      "source": [
        "## Conditionals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gXEAWFZfDTT"
      },
      "source": [
        "As their name indicates, conditionals are a way to execute code depending on whether a condition is True or False. As in other languages, Python supports `if` and `else` but `else if` is contracted into `elif`, as the example below demonstrates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xC_DMZjofoYZ",
        "outputId": "e86016aa-0ebd-4e0f-e559-326f4b2ce644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "my_variable = 5\n",
        "if my_variable < 0:\n",
        "  print(\"negative\")\n",
        "elif my_variable == 0:\n",
        "  print(\"null\")\n",
        "else: # my_variable > 0\n",
        "  print(\"positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag0SUokSf9jl"
      },
      "source": [
        "Here `<` and `>` are the strict `less` and `greater than` operators, while `==` is the equality operator (not to be confused with `=`, the variable assignment operator). The operators `<=` and `>=` can be used for less (resp. greater) than or equal comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTkQ2F_jy8wz"
      },
      "source": [
        "Contrary to other languages, blocks of code are delimited using indentation. Here, we use 2-space indentation but many programmers also use 4-space indentation. Any one is fine as long as you are consistent throughout your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clWaFCzBMfkv"
      },
      "source": [
        "## Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A5doqhTivWe"
      },
      "source": [
        "Loops are a way to execute a block of code multiple times. There are two main types of loops: while loops and for loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN8lwTxQkGEa"
      },
      "source": [
        "While loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "7-QXGqgOjsr_",
        "outputId": "4a9bdffc-46ba-47ba-e60b-3ea01dcd2d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "while i < len(my_list):\n",
        "  print(my_list[i])\n",
        "  i += 1 # equivalent to i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mEI_ocfkSvZ"
      },
      "source": [
        "For loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "2QObx5mckMcI",
        "outputId": "32c31a79-ef89-4e80-9e76-47540f839cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(my_list)):\n",
        "  print(my_list[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO6qqppikZvm"
      },
      "source": [
        "If the goal is simply to iterate over a list, we can do so directly as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "PjFKzN6zkeJ7",
        "outputId": "41f3a553-b629-4e52-ad98-7bbe55e0cced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "for element in my_list:\n",
        "  print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cck4zwYrex02"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1PbIf_ohxFO"
      },
      "source": [
        "To improve code readability, it is common to separate the code into different blocks, responsible for performing precise actions: functions. A function takes some inputs and process them to return some outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cImA09gOhRmx",
        "outputId": "85834282-56d6-4c17-f188-433f46c50d21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def square(x):\n",
        "  return x ** 2\n",
        "\n",
        "def multiply(a, b):\n",
        "  return a * b\n",
        "\n",
        "# Functions can be composed.\n",
        "square(multiply(3, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-5SOk9iYSt"
      },
      "source": [
        "To improve code readability, it is sometimes useful to explicitly name the arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "wkIUuZHhidI0",
        "outputId": "acb68380-db78-491c-ce27-8a4664f78ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "square(multiply(a=3, b=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkpwbQEVMys2"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpVhol9ZXI0"
      },
      "source": [
        "**Exercise 1.** Using a conditional, write the [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) function defined as follows\n",
        "\n",
        "$\\text{relu}(x) = \\left\\{\n",
        "   \\begin{array}{rl}\n",
        "     x, & \\text{if }  x \\ge 0 \\\\\n",
        "     0, & \\text{otherwise }.\n",
        "   \\end{array}\\right.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlgyu65SaUvr"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  # Write your function here\n",
        "  return\n",
        "\n",
        "relu(-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3so0ceoakIw"
      },
      "source": [
        "**Exercise 2.** Using a foor loop, write a function that computes the [Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of a vector, represented as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-IH-BD41bI1u",
        "outputId": "f45bf668-d55b-494d-ecd7-2f421cbacf15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.729746940310715"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def euclidean_norm(vector):\n",
        "  # Write your function here\n",
        "  return\n",
        "\n",
        "import numpy as np\n",
        "my_vector = [0.5, -1.2, 3.3, 4.5]\n",
        "# The result should be roughly 5.729746940310715\n",
        "euclidean_norm(my_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEXIh_e9cW3S"
      },
      "source": [
        "**Exercise 3.** Using a for loop and a conditional, write a function that returns the maximum value in a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd9ntMq0cb2e"
      },
      "outputs": [],
      "source": [
        "def vector_maximum(vector):\n",
        "  # Write your function here\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPAZA4OMc6sT"
      },
      "source": [
        "**Bonus exercise.** if time permits, write a function that sorts a list in ascending order (from smaller to bigger) using the [bubble sort](https://en.wikipedia.org/wiki/Bubble_sort) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBokdJO4dGyf"
      },
      "outputs": [],
      "source": [
        "def bubble_sort(my_list):\n",
        "  # Write your function here\n",
        "  return\n",
        "\n",
        "my_list = [1, -3, 3, 2]\n",
        "# Should return [-3, 1, 2, 3]\n",
        "bubble_sort(my_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDxjvtEEM1vg"
      },
      "source": [
        "## Going further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkmvzf-PdEp"
      },
      "source": [
        "Clearly, it is impossible to cover all the language features in this short introduction. To go further, we recommend the following resources:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X4WJo3iM6m9"
      },
      "source": [
        "# NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H3bNbLloXCY"
      },
      "source": [
        "NumPy is a popular library for storing arrays of numbers and performing computations on them. Not only this enables to write often more succint code, this also makes the code faster, since most NumPy routines are implemented in C for speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7tI3XLhqwSX"
      },
      "source": [
        "To use NumPy in your program, you need to import it as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phSPPyfyq2gX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9secCfFLNHEE"
      },
      "source": [
        "## Array creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSS2wEnkq97n"
      },
      "source": [
        "NumPy arrays can be created from Python lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Hfeg286yrLvJ",
        "outputId": "f498bafc-0373-4258-f479-e27716b193c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_array = np.array([1, 2, 3])\n",
        "my_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy2EvrxFriAG"
      },
      "source": [
        "NumPy supports array of arbitrary dimension. For example, we can create two-dimensional arrays (e.g. to store a matrix) as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "wM-GYVMsrzNs",
        "outputId": "3d524d20-cf2c-4d3c-ba08-4552be3a46d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_2d_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "my_2d_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kZMzYsAsVAc"
      },
      "source": [
        "We can access individual elements of a 2d-array using two indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4q8X86BbscPd",
        "outputId": "3cefe32a-690a-4744-fad9-9c3edd763cd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_2d_array[1, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfVIKyxkTh0p"
      },
      "source": [
        "We can also access rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CrKnDAtyTlYe",
        "outputId": "14060c28-8ee7-48f5-f0ca-bdcff12cc421"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 5, 6])"
            ]
          },
          "execution_count": 66,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_2d_array[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskLBCp9ToCG"
      },
      "source": [
        "and columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MOOFsLHhTozX",
        "outputId": "8802a59a-0812-40a4-f930-ab2a3302bf46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 6])"
            ]
          },
          "execution_count": 67,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_2d_array[:, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keWK_5PHr9Q2"
      },
      "source": [
        "Arrays have a `shape` attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5QIo7l1Yr8m7",
        "outputId": "357d4218-541d-4c7f-8bcf-b3f4523b1fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(2, 3)\n"
          ]
        }
      ],
      "source": [
        "print(my_array.shape)\n",
        "print(my_2d_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmX0EDWVsoDY"
      },
      "source": [
        "Contrary to Python lists, NumPy arrays must have a type and all elements of the array must have the same type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FZjOowkls57o",
        "outputId": "abb3edab-a903-40c8-f88a-1125fc6d4dbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_array.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5AvLdf7tGnZ"
      },
      "source": [
        "The main types are `int32` (32-bit integers), `int64` (64-bit integers), `float32` (32-bit real values) and `float64` (64-bit real values)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ym2qZCt9Nm"
      },
      "source": [
        "The `dtype` can be specified when creating the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gXpM_FqruCVv",
        "outputId": "20d8b981-56ed-4458-d9d8-2f9fb5fb2b58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "execution_count": 32,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_array = np.array([1, 2, 3], dtype=np.float64)\n",
        "my_array.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WueaRIONuTdS"
      },
      "source": [
        "We can create arrays of all zeros using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "jbD8N1UauK8r",
        "outputId": "4a352f81-f0e8-4bc1-a651-760436c7d213"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zero_array = np.zeros((2, 3))\n",
        "zero_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn5go6qoudo4"
      },
      "source": [
        "and similarly for all ones using `ones` instead of `zeros`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kCRlhLJuvZ6"
      },
      "source": [
        "We can create a range of values using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EcQXDeEmuxpO",
        "outputId": "5a6fac79-26e3-4012-d5f4-82844372dc58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4])"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.arange(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvJECk6Iu3uF"
      },
      "source": [
        "or specifying the starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Pk3UzL3du_f8",
        "outputId": "1fd3f3fa-63ba-4a26-9274-011574bebbd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 4])"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.arange(3, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1JtqFSivJKG"
      },
      "source": [
        "Another useful routine is `linspace` for creating linearly spaced values in an interval. For instance, to create 10 values in `[0, 1]`, we can use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "udHHjGAHvOQM",
        "outputId": "473cec27-ac56-4dc9-984a-c1e340255a51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
              "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])"
            ]
          },
          "execution_count": 36,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.linspace(0, 1, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbcxAKobvgUT"
      },
      "source": [
        "Another important operation is `reshape`, for changing the shape of an array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "4FPzTuDlvlLO",
        "outputId": "338648ec-11e3-436f-d0e4-6745f9cb30d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4],\n",
              "       [5, 6]])"
            ]
          },
          "execution_count": 37,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_array = np.array([1, 2, 3, 4, 5, 6])\n",
        "my_array.reshape(3, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-QR80_g3N9Y"
      },
      "source": [
        "Play with these operations and make sure you understand them well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9B0iCBlmfeY"
      },
      "source": [
        "## Basic operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQGgkqDxKLV"
      },
      "source": [
        "In NumPy, we express computations directly over arrays. This makes the code much more succint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkCU1T8ixghX"
      },
      "source": [
        "Arithmetic operations can be performed directly over arrays. For instance, assuming two arrays have a compatible shape, we can add them as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4AoiRq42x5mI",
        "outputId": "d9706493-95f8-43d5-af90-21a53c13cac5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array_a = np.array([1, 2, 3])\n",
        "array_b = np.array([4, 5, 6])\n",
        "array_a + array_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPqME2EyD4x"
      },
      "source": [
        "Compare this with the equivalent computation using a for loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HxRFA_U2yfI-",
        "outputId": "cbd77fed-8b67-4119-aeea-7a71d92b63b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "execution_count": 65,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array_out = np.zeros_like(array_a)\n",
        "for i in range(len(array_a)):\n",
        "  array_out[i] = array_a[i] + array_b[i]\n",
        "array_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2a-apX-zlPN"
      },
      "source": [
        "Not only this code is more verbose, it will also run much more slowly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdn8MwpR0wX_"
      },
      "source": [
        "In NumPy, functions that operates on arrays in an element-wise fashion are called [universal functions](https://numpy.org/doc/stable/reference/ufuncs.html). For instance, this is the case of `np.sin`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JoanjiMu1BH5",
        "outputId": "e6b8be44-7e66-4a3f-eb57-c767c7f8b2f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.84147098, 0.90929743, 0.14112001])"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sin(array_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHljrPXg5h8W"
      },
      "source": [
        "Vector inner product can be performed using `np.dot`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TphR8oIx5ob9",
        "outputId": "a84a8966-6b99-4a48-b422-caea4dd0ffc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 44,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.dot(array_a, array_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHInOiSW50OR"
      },
      "source": [
        "When the two arguments to `np.dot` are both 2d arrays, `np.dot` becomes matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "QRbpbhPP6Up0",
        "outputId": "d020a4d4-2532-495f-fd28-766fc92b3318"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.36045702, -0.81071381, -0.19270751,  1.68942764],\n",
              "       [-1.37444349, -3.05245084, -0.52466652, -0.02343348],\n",
              "       [-1.43277431, -2.95828896, -0.4035378 , -0.50852563],\n",
              "       [-0.8569399 , -0.99003545,  0.17051909,  1.02933425],\n",
              "       [-0.47198448, -1.52564526, -0.41890404, -1.29330023]])"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array_A = np.random.rand(5, 3)\n",
        "array_B = np.random.randn(3, 4)\n",
        "np.dot(array_A, array_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odVawD9m6gwv"
      },
      "source": [
        "Matrix transpose can be done using `.transpose()` or `.T` for short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "gvPe_JAO6mvF",
        "outputId": "4952000d-c255-4cd0-8b12-51bcb59ba53b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.83246658, 0.86545167, 0.62386601, 0.473339  , 0.06991272],\n",
              "       [0.02185012, 0.93435538, 0.93170156, 0.0036374 , 0.7230155 ],\n",
              "       [0.08128956, 0.83778882, 0.96709345, 0.66291745, 0.2734715 ]])"
            ]
          },
          "execution_count": 46,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array_A.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlWt3oFnE_E-"
      },
      "source": [
        "## Slicing and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4aKKe7bFA65"
      },
      "source": [
        "Like Python lists, NumPy arrays support slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0kPhv2xcF1TP",
        "outputId": "a7315cac-0495-44da-e497-63aa677b8d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 61,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.arange(10)[5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITu2Wy4-GB2G"
      },
      "source": [
        "We can also select only certain elements from the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8tlZzTB6GEyw",
        "outputId": "a2918cd3-f33b-48de-cd18-a3a8b1fb2fab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 62,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.arange(10)\n",
        "mask = x >= 5\n",
        "x[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlGForCimjBL"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur1UlSFPTu6O"
      },
      "source": [
        "**Exercise 1.** Create a 3d array of shape (2, 2, 2), containing 8 values. Access individual elements and slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1ed4-vLUWXQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ksfCDJzyxI"
      },
      "source": [
        "**Exercise 2.** Rewrite the relu function (see Python section) using [np.maximum](https://numpy.org/doc/stable/reference/generated/numpy.maximum.html). Check that it works on both a single value and on an array of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtSTxH5Dz6f8"
      },
      "outputs": [],
      "source": [
        "def relu_numpy(x):\n",
        "  return\n",
        "\n",
        "relu_numpy(np.array([1, -3, 2.5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wggUjpyRz7fb"
      },
      "source": [
        "**Exercise 3.** Rewrite the Euclidean norm of a vector (1d array) using NumPy (without for loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5BLcHOD0Bhy"
      },
      "outputs": [],
      "source": [
        "def euclidean_norm_numpy(x):\n",
        "  return\n",
        "\n",
        "my_vector = np.array([0.5, -1.2, 3.3, 4.5])\n",
        "euclidean_norm_numpy(my_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01IteVJ60Il2"
      },
      "source": [
        "**Exercise 4.** Write a function that computes the Euclidean norms of a matrix (2d array) in a row-wise fashion. Hint: use the `axis` argument of [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at5lWRNM0SVG"
      },
      "outputs": [],
      "source": [
        "def euclidean_norm_2d(X):\n",
        "  return\n",
        "\n",
        "my_matrix = np.array([[0.5, -1.2, 4.5],\n",
        "                      [-3.2, 1.9, 2.7]])\n",
        "# Should return an array of size 2.\n",
        "euclidean_norm_2d(my_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1ZoByo436x"
      },
      "source": [
        "**Exercise 5.** Compute the mean value of the features in the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Hint: use the `axis` argument on [np.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "fYFVobkP5JK6",
        "outputId": "91504ff7-ad59-4eb7-f940-cb2e068e1b6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Result should be an array of size 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FDs9zX6mpoX"
      },
      "source": [
        "## Going further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFP61Iztmr9Q"
      },
      "source": [
        "* NumPy [reference](https://numpy.org/doc/stable/reference/)\n",
        "* SciPy [lectures](https://scipy-lectures.org/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jt6T3kJ8I2T"
      },
      "source": [
        "# Matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQX8TiEOALkQ"
      },
      "source": [
        "## Basic plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REYwc9Va8UTg"
      },
      "source": [
        "Matplotlib is a plotting library for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eom7t-m6-Uzb"
      },
      "source": [
        "We start with a rudimentary plotting example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "g21e5Ncm927z",
        "outputId": "0bffba14-376e-4274-a33a-04cc8ce6cd64"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x_values = np.linspace(-3, 3, 100)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x_values, np.sin(x_values), label=\"Sinusoid\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"sin(x)\")\n",
        "plt.title(\"Matplotlib example\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltvlLwXF-eAH"
      },
      "source": [
        "We continue with a rudimentary scatter plot example. This example displays samples from the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) using the first two features. Colors indicate class membership (there are 3 classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "sEzcJAmy-hbK",
        "outputId": "7d965d22-d0b6-41d5-9f80-f5d6a7d7f93f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "X_class0 = X[y == 0]\n",
        "X_class1 = X[y == 1]\n",
        "X_class2 = X[y == 2]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_class0[:, 0], X_class0[:, 1], label=\"Class 0\", color=\"C0\")\n",
        "plt.scatter(X_class1[:, 0], X_class1[:, 1], label=\"Class 1\", color=\"C1\")\n",
        "plt.scatter(X_class2[:, 0], X_class2[:, 1], label=\"Class 2\", color=\"C2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vjln9qwAc3M"
      },
      "source": [
        "We see that samples belonging to class 0 can be linearly separated from the rest using only the first two features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVWuIUs2AQ5a"
      },
      "source": [
        "## Exercises\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X6-g6zgCwJd"
      },
      "source": [
        "**Exercise 1.** Plot the relu and the [softplus](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Softplus) functions on the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob6HZUX0DJ8y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpRGfz0aDW3l"
      },
      "source": [
        "What is the main difference between the two functions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDeIufRAYVL"
      },
      "source": [
        "**Exercise 2.** Repeat the same scatter plot but using the [digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JU3TXCBBB0c"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X, y = load_digits(return_X_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7wPWdmXBQA2"
      },
      "source": [
        "Are pixel values good features for classifying samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYM-oV1jD3RV"
      },
      "source": [
        "## Going further\n",
        "\n",
        "*  Official [tutorial](https://matplotlib.org/tutorials/introductory/pyplot.html)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": " Python basics",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}